{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408141cd-57bc-4c1a-9fa9-77c4d4bbfc87",
   "metadata": {},
   "source": [
    "# Gerekli Kütüphaneleri Import Ediyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589a4f51-808c-4ef8-bffa-dce4e1e2a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from seaborn import histplot\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f348dc-1caf-4311-bad1-b0880513659f",
   "metadata": {},
   "source": [
    "## Directions\n",
    "Directionları giriyorum. İlk başlarda dataseti import ederken hata aldığım için izinleri kontrol ediyorum.  \n",
    "2. train_dir'in sebebi kodu daha hızlı çalıştırmak ve bug fix aşamasını daha hızlı yapmak için dataseti ayırmam. orijinal olan dataset 'original' klasörü altında."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab896ba-a244-419f-85c3-f0e6461bab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(os.getcwd(), 'brain-tumor')\n",
    "train_dir = os.path.join(train_dir, 'train')\n",
    "#train_dir = os.path.join(train_dir, 'original')\n",
    "images_dir = os.path.join(train_dir, 'images')\n",
    "labels_dir = os.path.join(train_dir, 'labels')\n",
    "print(os.access(images_dir, os.R_OK))  # Check read permission\n",
    "print(os.access(images_dir, os.W_OK))  # Check write permission\n",
    "print(os.access(images_dir, os.X_OK))  # Check execute permission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54091c12-92f3-49cd-a736-9918c46aca36",
   "metadata": {},
   "source": [
    "## Karşılıksız Dosyaları Silme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985d93f7-8070-4fba-a2f9-5c441df070f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(file.split('.')[0] for file in os.listdir(labels_dir))\n",
    "images = set(file.split('.')[0] for file in os.listdir(images_dir))\n",
    "\n",
    "for file in images - labels:\n",
    "    os.remove(os.path.join(images_dir, file + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d897df-c8e4-43ce-a4c5-14342f8124c0",
   "metadata": {},
   "source": [
    "# Dataset import class'ı\n",
    "image'lerin hepsi jpg ile bittiği için diğer uzantıları eklemedim.  \n",
    "\n",
    "getitem'da image'leri siyah-beyaz olarak istedim. Label'larda ise ilk değeri aldım. batch için unsqueeze ile ekstra dimension ekledim. ileride squeeze kullanacağım. Örnek olarak:  \n",
    "> label = 1\n",
    "> 1. int(1) -> 1\n",
    "> 2. torch.tensor(1, dtype=torch.float32) -> tensor(1.0)\n",
    "> 3. tensor(1.0).unsqueeze(0) -> tensor([1.0]) (shape (1,))\n",
    "\n",
    "Olası hata durumlarını ayıklamak için try except ile düzeltmelerimi yaptım. Başta dediğim gibi directionlardan kaynaklanan sorunlar aldım, izinleri kontrol etmek işe yaradı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69853b44-086a-4612-8fb9-c0a19b7e25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_filenames = sorted([os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.lower().endswith(('.jpg'))])\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.images_dir, img_name + '.jpg')\n",
    "        label_path = os.path.join(self.labels_dir, img_name + '.txt')\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.convert('L') # ensure black and white\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                label = int(f.read().strip()[0])  # Convert '0'/'1' to integer\n",
    "            label = torch.tensor(int(label), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image or label file not found for {img_name}. Skipping.\")\n",
    "            return self.__getitem__((idx + 1) % len(self)) # Return the next item (handles edge cases)\n",
    "        except Exception as e: # Catch other potential issues\n",
    "            print(f\"Error loading image or label for {img_name}: {e}. Skipping.\")\n",
    "            return self.__getitem__((idx + 1) % len(self)) # Return the next item (handles edge cases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2989a-7d2d-435a-a12a-b65ec32b293e",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Transformation'ları tanımladm.  \n",
    "Batch ayarlarını yaptım ve train_loader ile uygun hale getirdim.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b54a9e6-0e96-4d33-933f-74825e4634c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([1.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([0.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([0.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([0.])\n",
      "Image shape: torch.Size([1, 512, 512]), Label: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ensure correct size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n",
    "])\n",
    "\n",
    "dataset = BrainTumorDataset(images_dir, labels_dir, transform=data_transforms)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Accessing data:\n",
    "for i in range(len(dataset)):\n",
    "    image, label = dataset[i]\n",
    "    print(f\"Image shape: {image.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6936c6bc-dcfd-44cb-81e5-ca5fe0afeafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_labels_np = torch.tensor([label for _, label in dataset]).numpy()\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Histogram visualization to see frequency distribution\n",
    "plt.hist(_labels_np, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Value Range\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413265b6-8fc3-45ad-b429-01351349b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self): # Will not add in channels parameter, assuming black and white which only has 1 channel\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "\n",
    "        # 16 out channels, 128x128 remaining res.\n",
    "        self.fc1 = nn.Linear(16 * 128 * 128, 128)  \n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x)) # First step\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = nn.ReLU()(self.conv2(x)) # Second step\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 16 * 128 * 128)  # Flatten\n",
    "        # safer alternatives:\n",
    "        # x = x.view(x.shape[0], -1)\n",
    "        # x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc93c309-c997-45e0-a2ff-2c6be8110db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f234391f-2fa6-403d-ae9e-5263e98b34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss Function\n",
    "criterion  = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75957296-b78f-40c0-96db-8324f62aaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6b5a6-42c6-4889-b8e3-e512429d60aa",
   "metadata": {},
   "source": [
    "### num_epochs\n",
    "num_epochs'u farklı farklı sayılarda denememe rağmen her zaman ilk değere eşit geldi. O yüzden 1 yapacaktım fakat farklılık olma ihtimaline karşılık gözlemliyim diye 2 yapıyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b177d3a-6a84-402d-81b1-4e8c4fd74001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 1.3133\n",
      "Epoch [2/2], Loss: 1.3133\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in dataset:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images).squeeze(1)  # Forward pass\n",
    "        labels = labels.view(-1)  # Ensure labels have shape [batch_size]\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        \n",
    "        optimizer.zero_grad()   # Clear previous gradients\n",
    "        loss.backward()  # Backpropagation (compute gradients)\n",
    "        \n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a5ee25-e353-4f26-9a3d-f20a11b7e642",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels, all_preds)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, f1\n\u001b[1;32m---> 25\u001b[0m accuracy, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_classification_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mbinary_classification_metrics\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Apply sigmoid to get probabilities\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         preds \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Thresholding at 0.5 for binary classification\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move to CPU and append to list\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Same for labels\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and F1 score\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "def binary_classification_metrics(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Get predictions from CNN\n",
    "            outputs = model(images)  # Forward pass through the CNN\n",
    "\n",
    "            # Apply sigmoid to logits and threshold to get binary predictions\n",
    "            probs = torch.sigmoid(outputs).squeeze()  # Apply sigmoid to get probabilities\n",
    "            preds = (probs > 0.5).float()  # Thresholding at 0.5 for binary classification\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())  # Move to CPU and append to list\n",
    "            all_labels.extend(labels.cpu().numpy())  # Same for labels\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, f1\n",
    "accuracy, f1 = binary_classification_metrics(model, dataset, device)\n",
    "print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dd082-9a13-405f-8cd5-ccb841d8af82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
